{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "202e14e8-fe56-4714-8f87-2c71e1c51e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv1D, Flatten, LSTM, Dense, Dropout, BatchNormalization, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings('ignore', message='SymbolDatabase.GetPrototype() is deprecated.')\n",
    "\n",
    "# Set the base directory for videos\n",
    "base_dir = r'C:\\Users\\AnandaMuthu\\Downloads\\aaasaturday\\dataset'\n",
    "\n",
    "# Action names\n",
    "action_names = [\"Bhujangasana\",\"Padmasana\",\"Shavasana\",\"Tadasana\",\"Trikonasana\",\"Vrikshasana\"]\n",
    "\n",
    "# Map labels to numeric values\n",
    "label_map = {name: i for i, name in enumerate(action_names)}\n",
    "\n",
    "# Function to get all video paths and their labels\n",
    "def get_video_paths_and_labels(base_dir):\n",
    "    video_paths = []\n",
    "    labels = []\n",
    "    for action in os.listdir(base_dir):\n",
    "        action_dir = os.path.join(base_dir, action)\n",
    "        if os.path.isdir(action_dir):\n",
    "            for video in os.listdir(action_dir):\n",
    "                if video.endswith(\".mp4\"):\n",
    "                    video_paths.append(os.path.join(action_dir, video))\n",
    "                    labels.append(action)\n",
    "    return video_paths, labels\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_video = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, model_complexity=1)\n",
    "\n",
    "# Function to perform pose detection and return annotated frame and landmarks\n",
    "def detect_pose(frame, pose_video):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose_video.process(frame_rgb)\n",
    "    if results.pose_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(frame, results.pose_landmarks, mp.solutions.pose.POSE_CONNECTIONS)\n",
    "    return frame, results\n",
    "\n",
    "# Function to analyze the results and get coordinates of each landmark\n",
    "def get_landmark_coordinates(results):\n",
    "    if not results.pose_landmarks:\n",
    "        return []\n",
    "    landmarks = []\n",
    "    for lm in results.pose_landmarks.landmark:\n",
    "        landmarks.append((lm.x, lm.y, lm.z))\n",
    "    return landmarks\n",
    "\n",
    "# Function to process video, extract frames, perform pose estimation, and collect results\n",
    "def process_video(video_path, skip_frames=5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return []\n",
    "    \n",
    "    frame_number = 0\n",
    "    all_coordinates = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_number % skip_frames == 0:\n",
    "            # Perform pose detection\n",
    "            _, results = detect_pose(frame, pose_video)\n",
    "            coordinates = get_landmark_coordinates(results)\n",
    "            \n",
    "            # Store the coordinates\n",
    "            if coordinates:\n",
    "                all_coordinates.append(coordinates)\n",
    "\n",
    "        frame_number += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return all_coordinates\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(all_coordinates_array, all_labels_array, sequence_length, num_joints):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(all_coordinates_array) - sequence_length + 1):\n",
    "        frame_sequence = np.array(all_coordinates_array[i:i + sequence_length])\n",
    "        sequences.append(frame_sequence.reshape((sequence_length, num_joints, 3)))  # 3 coordinates (x, y, z)\n",
    "        labels.append(label_map[all_labels_array[i]])\n",
    "    sequences = np.array(sequences)\n",
    "    labels = to_categorical(labels, num_classes=len(action_names))\n",
    "    return sequences, labels\n",
    "\n",
    "# Function to build the CNN-LSTM model\n",
    "def build_model(num_joints, num_classes, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(sequence_length, num_joints, 3)))  # Input layer\n",
    "    model.add(TimeDistributed(Conv1D(filters=16, kernel_size=3, activation='relu')))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(64, return_sequences=False))  # Adjust LSTM units as needed\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb37d86-e8d9-4168-abed-94f297227c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AnandaMuthu\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "# Get video paths and labels\n",
    "video_paths, labels = get_video_paths_and_labels(base_dir)\n",
    "all_coordinates_array = []\n",
    "all_labels_array = []\n",
    "\n",
    "# Process each video to extract pose landmarks\n",
    "for video_path, label in zip(video_paths, labels):\n",
    "    coordinates = process_video(video_path, skip_frames=5)\n",
    "    if coordinates:\n",
    "        all_coordinates_array.extend(coordinates)\n",
    "        all_labels_array.extend([label] * len(coordinates))\n",
    "\n",
    "num_joints = 33  # Number of landmarks detected by MediaPipe Pose\n",
    "sequence_length = 30  # Number of frames to consider in a sequence\n",
    "\n",
    "# Preprocess data\n",
    "if len(all_coordinates_array) > sequence_length:\n",
    "    sequences, labels = preprocess_data(all_coordinates_array, all_labels_array, sequence_length, num_joints)\n",
    "    \n",
    "    # Build the model\n",
    "    model = build_model(num_joints, len(action_names), sequence_length)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=25, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(\"yogapose_detection_model.keras\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(classification_report(y_true, y_pred_classes, target_names=action_names))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=action_names, yticklabels=action_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: Not enough data to generate sequences. Ensure you have enough frames and proper input format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6fa36-dd79-46fe-8ead-bd25daf4227a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c44a64-d38e-4629-8459-0b88a1677548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
